# Multidimensional Geometry Evaluation Project

Evaluate LLM performance on geometry questions across 2D, 3D, and 4D geometric shapes.

## Repository Structure

### Git-Tracked Files

#### Root Level
- **`evaluate.py`** — Main evaluation script. Runs multiple-choice and numeric geometry questions against LLMs, computes accuracy per question type, generates confusion matrices, and logs results.


#### Data Directory (`data/`)
- **`questions.csv`** — Original multiple-choice geometry questions. Columns: `2D`, `3D`, `4D`, `answer`, `type`.
  - `2D/3D/4D`: Question text for each dimension.
  - `answer`: Correct answer (A, B, C, or D).
  - `type`: Question category (1, 2, or 3).
        - Type 1: Parallel/Perpendicular Classification
        - Type 2: Intersection Classification
        - Type 3: Collinearity Classification
- **`numeric.csv`** — Integer-answer geometry questions. Columns: `question`, `dimension`, `answer`.
- **`questions_augmented.csv`** — Augmented questions generated by permuting vertex labels. Additional columns: `aug_source_id`, `aug_map`.

#### Scripts Directory (`scripts/`)
- **`augment_questions.py`** — Dataset augmentation script. Permutes uppercase vertex labels (A–Z, excluding O, I, l, 0) to generate variations of existing questions. Outputs `data/questions_augmented.csv`.
- **`augment_numeric.py`** — Numeric-question augmentation utilities (creates/augments `data/numeric_augmented.csv`).
- **`run.sh`** — Example run wrapper (sanitized) to run `evaluate.py` with common options. Contains placeholders for Azure credentials and CUDA device selection.

---

### Gitignored Directories

- **`results/`** — Evaluation output directory. Contains timestamped subdirectories with confusion matrices (PNG) and result summaries per model run.
- **`past_results/`** — Archive of previous evaluation runs.
- **`logs/`** — Log files (if generated).
- **`__pycache__/`** — Python bytecode cache.
- **`scripts/run.sh`** — Local helper script (sanitized). Keep secrets out of source and update environment variables before use.

---

## Usage
### Running Evaluations

The evaluation script now provides a small CLI. From the repository root you can run the script
in the `master/` directory:

```bash
# If evaluating hosted Azure models, export your Azure credentials first:
export AZURE_OPENAI_API_KEY="<your_api_key>"
export AZURE_OPENAI_ENDPOINT="<your_endpoint>"
export AZURE_OPENAI_API_VERSION="<your_api_version>"

# Run with defaults (built-in model list, 2D):
python master/evaluate.py

# Evaluate a single local vLLM model for 2D and 3D:
python master/evaluate.py --models Qwen/Qwen2.5-7B-Instruct --dims 2,3

# Multiple models, custom batch size and token limit:
python master/evaluate.py --models Qwen/Qwen2.5-7B-Instruct,gpt-4o --dims 2 --batch-size 4 --max-new-tokens 512

# Save results under a custom root and explicit timestamp:
python master/evaluate.py --results-root my_results --timestamp 20251225_120000
```

Notes:
- Local models specified in `--models` (e.g. `Qwen/...`) are loaded via `vllm.LLM`.
- Hosted/OpenAI-style models (e.g. `gpt-4o`, `gpt-5`) are queried through the Azure OpenAI client when
  the script is run without a local `vllm` model for that name. Provide Azure environment variables to enable this.

Key CLI options:
- `--models`: comma-separated model names to evaluate (default: built-in list in the script)
- `--dims`: comma-separated dimensions to evaluate (default: `2`)
- `--batch-size`: batch size for local generation (default: `8`)
- `--max-new-tokens`: maximum tokens to generate per prompt (default: `1024`)
- `--no-reasoning`: disable chain-of-thought reasoning prompts
- `--results-root`: base directory for results (default: `results`)
- `--timestamp`: override the run timestamp used to name output folders

Results are saved to:
```
{results_root}/{timestamp}/{model_name}/
├── results.text              # Summary statistics
├── confusion_matrix_2d.png   # 2D confusion matrices (if applicable)
├── confusion_matrix_3d.png   # 3D confusion matrices (if applicable)
└── confusion_matrix_4d.png   # 4D confusion matrices (if applicable)
```

### Augmenting Questions

```bash
python scripts/augment_questions.py
```

This generates `data/questions_augmented.csv` with augmented variants.

---

## File Purposes & Gitignore Rationale

| File/Dir | Git-Tracked | Reason |
|----------|:-----------:|--------|
| `evaluate.py` | ✓ | Core evaluation logic; versioned for reproducibility |
| `data/*.csv` | ✓ | Source datasets (questions and numeric answers) |
| `scripts/augment_questions.py` | ✓ | Data generation code |
| `results/` | ✗ | Large, generated outputs (PNG images + logs). Not needed in repo; stored locally per run. |
| `past_results/` | ✗ | Archive of previous runs. Local-only for reference. |
| `logs/` | ✗ | Runtime logs; local debugging only. |
| `__pycache__/` | ✗ | Python build artifacts; OS-specific. |
| `scripts/run.sh` | ✗ | Local helper script (contains machine-specific env vars). |

---

## Output Format

Results are saved with a session timestamp (e.g., `20251224_143022`) and organized by model name:
- Per-dimension accuracy and confusion matrices
- Per-type (question category) breakdowns
- Overall accuracy summary in `results.text`

All PNG plots are automatically generated with proper labeling and saved in the results directory.
