# Multidimensional Geometry Evaluation Project

Evaluate LLM performance on geometry questions across 2D, 3D, and 4D geometric shapes.

## Repository Structure

### Git-Tracked Files

#### Root Level
- **`evaluate.py`** — Main evaluation script. Runs multiple-choice and numeric geometry questions against LLMs, computes accuracy per question type, generates confusion matrices, and logs results.


#### Data Directory (`data/`)
- **`questions.csv`** — Original multiple-choice geometry questions. Columns: `2D`, `3D`, `4D`, `answer`, `type`.
  - `2D/3D/4D`: Question text for each dimension.
  - `answer`: Correct answer (A, B, C, or D).
  - `type`: Question category (1, 2, or 3).
        - Type 1: Parallel/Perpendicular Classification
        - Type 2: Intersection Classification
        - Type 3: Collinearity Classification
- **`numeric.csv`** — Integer-answer geometry questions. Columns: `question`, `dimension`, `answer`.
- **`questions_augmented.csv`** — Augmented questions generated by permuting vertex labels. Additional columns: `aug_source_id`, `aug_map`.

#### Scripts Directory (`scripts/`)
- **`augment_questions.py`** — Dataset augmentation script. Permutes uppercase vertex labels (A–Z, excluding O, I, l, 0) to generate variations of existing questions. Outputs `data/questions_augmented.csv`.

---

### Gitignored Directories

- **`results/`** — Evaluation output directory. Contains timestamped subdirectories with confusion matrices (PNG) and result summaries per model run.
- **`past_results/`** — Archive of previous evaluation runs.
- **`logs/`** — Log files (if generated).
- **`__pycache__/`** — Python bytecode cache.
- **`run.sh`** — Local shell script (excluded to avoid accidentally committing local-specific commands).

---

## Usage

### Running Evaluations

```bash
# if you use GPT models, you need to set your AZURE environment variables first
export AZURE_OPENAI_API_KEY="<your_api_key>"
export AZURE_OPENAI_ENDPOINT="<your_endpoint>"
export AZURE_OPENAI_API_VERSION="<your_api_version>"

CUDA_VISIBLE_DEVICES=0 python evaluate.py
```

Results are saved to:
```
results/{timestamp}/{model_name}/
├── results.text              # Summary statistics
├── confusion_matrix_2d.png   # 2D confusion matrices
├── confusion_matrix_3d.png   # 3D confusion matrices
└── confusion_matrix_4d.png   # 4D confusion matrices
```

### Augmenting Questions

```bash
python scripts/augment_questions.py
```

This generates `data/questions_augmented.csv` with augmented variants.

---

## File Purposes & Gitignore Rationale

| File/Dir | Git-Tracked | Reason |
|----------|:-----------:|--------|
| `evaluate.py` | ✓ | Core evaluation logic; versioned for reproducibility |
| `data/*.csv` | ✓ | Source datasets (questions and numeric answers) |
| `scripts/augment_questions.py` | ✓ | Data generation code |
| `results/` | ✗ | Large, generated outputs (PNG images + logs). Not needed in repo; stored locally per run. |
| `past_results/` | ✗ | Archive of previous runs. Local-only for reference. |
| `logs/` | ✗ | Runtime logs; local debugging only. |
| `__pycache__/` | ✗ | Python build artifacts; OS-specific. |
| `run.sh` | ✗ | Local test/debug scripts that may vary per machine. |

---

## Output Format

Results are saved with a session timestamp (e.g., `20251224_143022`) and organized by model name:
- Per-dimension accuracy and confusion matrices
- Per-type (question category) breakdowns
- Overall accuracy summary in `results.text`

All PNG plots are automatically generated with proper labeling and saved in the results directory.
